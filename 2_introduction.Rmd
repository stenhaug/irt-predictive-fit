A focal point of psychological measurement is item response data generated when persons respond to items (e.g., multiple choice items in educational assessments). Item response models are statistical models fit to such item response data. As with most statistical models, more and less flexible versions of item response models are available. Consider the common family of unidimensional models dichotomous item responses: The one-parameter logistic (1PL), the two-parameter (2PL) logistic, and the three-parameter (3PL) logistic models. The 1PL model is the least flexible, with just a difficulty parameter for each item [@rasch1960probabilistic]. The 3PL model is the most flexible, with a difficulty, discrimination, and guessing parameter for each item [@birnbaum1968some].

Suppose that data is generated by a 3PL model (i.e., the data-generating model, abbreviated "DGM") and that both a 2PL model and 3PL model are estimated using this data. What does it mean for one of these models to "fit" the data? In item response theory research literature, fit is often defined by whether the model could have produced the data [@ditrapani2019assessing]. In essence, a variety of methods aim to test the null hypothesis that the model generated the data. For example, the M2 statistic compares the expected (according to the model) to the observed (by counting the data) moments of a contingency table [@maydeu2005limited]. If there is a mismatch between these moments, then the null hypothesis that the model could have generated the data is rejected and it is concluded that the model has poor fit.^[This comparison can also be translated into a goodness-of-fit metric such RMSEA$_2$ [@steiger1990structural].] Similarly, posterior predictive checks use the model to simulate data, use discrepancy measures to compare that simulated data to the observed data, and then conclude whether the model could have produced the observed data based on those discrepancy measures [@sinharay2006posterior].

Item response model simulation studies, which are commonly used to guide usage in empirical settings, often take a similar view of fit. @luecht2018technical summarize the majority of these simulation studies as following the \emph{comparative model fit} script, wherein (1) a DGM model is chosen (e.g., the 3PL), (2) item and person parameters are specified and item response data is simulated, (3) a variety of models are estimated using the simulated data, and (4) those models are compared. @luecht2018technical point out the inevitable conclusion that the model with the same parameterization as the DGM best fits the data. Going a step further, they remark that "one might even conclude that that result is axiomatic, thus eliminating the need to ever again again see this type of IRT simulation study published" (p. 66).

As an example of such a simulation study, consider @kang2007irt who evaluated the effectiveness of a variety of item response model comparison methods such as Akaike's information criterion [AIC; @akaike1974new] and Bayesian information criterion [BIC; @schwarz1978estimating]. They simulated data via the 3PL DGM, and fit 1PL, 2PL, and 3PL models to the simulated data. Finally, and this is crucial, they evaluated a model comparison method's (e.g., BIC) performance according to its ability to choose the 3PL model as the best fitting model. Their implicit assumption was that, by definition, if a 3PL model generated the data, then a 3PL model must best fit the data. After all, that model produced the data. One of their conclusions was that BIC performed poorly for data generated from a 3PL model because BIC preferred a simpler (than the 3PL) model. Other research on model comparison methods has also assumed that the prediction-maximizing model shares the same parameterization as the DGM: For example, @svetina2016dimensionality negatively judged NOHARM, a method for detecting the dimensionality of item response data, based on its tendency to find fewer than the data-generating number of dimensions at low sample sizes.

## An Alternative Approach to Fit: Predictive Fit

We have summarized this previous work to illustrate how fit is often conceptualized in the item response theory research literature. An alternative approach, which has gained traction in computer science, is predictive fit which is based on how well a model predicts new (i.e., out-of-sample) data from the DGM [@gelman2014understanding]. The fundamental logic of predictive fit is that the model with the best predictions is likely to be the most useful. When introducing the now eponymous model, @rasch1960probabilistic wrote, "When you construct a model you leave out all the details\ldots{} Models should not be true, but it is important that they are \emph{applicable}" (p. 38). @lord1983small argued that the Rasch model should be preferred at small sample sizes, even if it is known to be the "wrong" model, precisely because it might offer better predictions. At the core of both Lord and Rasch's observation is that the goal of model selection should be to identify a useful model—which can be measured by quality of predictions—not necessarily the "right" or "true" model. 

We have summarized this previous work to illustrate how fit is often conceptualized in the item response theory research literature. An alternative approach, which has gained traction in computer science, is predictive fit [@gelman2014understanding]. The fundamental logic of predictive fit is that the model with the best predictions is likely to be the most useful. @box1976science famously wrote that "all models are wrong" [p. 66]. Perhaps less famously, @rasch1960probabilistic, in the same text that introduced the Rasch model, wrote, "When you construct a model you leave out all the details... Models should not be true, but it is important that they are *applicable*" [p. 38]. @lord1983small argued that the Rasch model should be preferred at small sample sizes, even if it is known to be the "wrong" model, precisely because it might offer better predictions. Indeed, a compelling way to assess how applicable or useful an item response model is by the quality of its predictions. Following @gelman2014understanding, we define the predictive fit of an item response model by how well it predicts *new* data from the DGM. As is common, we refer to new data as out-of-sample data, as compared to in-sample data which was used to estimate the model's parameters.

One might argue that item response models are typically used to explain and understand as opposed to predict. We agree with the many who have argued that this is a false dichotomy and that explanation and prediction are in fact two sides of the same coin [@watts2018explanation; @yarkoni2017choosing]. @feynman1965feynman argued that science is  the process of making hypotheses and checking their predictions against new data. In other words, the ultimate test of a scientific hypothesis is its ability to make predictions. Focusing on item response data, we should not trust conclusions from item response models that make poor predictions. In fact, in operational settings, one cannot know that the data came from an item response model; the question is rather whether item response models can characterize the data usefully. The better an item response model's predictions, the better it has characterized the data and the more trust we can place in its conclusions. Further, we argue that many item response model simulation studies would be more valuable if they assessed models according to their predictive fit. The predictive fit view argues that it's better to have a model that produces high-quality predictions than it is to have a model with the same parameterization as the DGM. Thus, @kang2007irt might have judged model selection methods not by their ability to identify the DGM, but instead by their ability to select the model that makes the best predictions.

In essence, we argue that a compelling way to assess how applicable or useful an item response model is by the quality of its predictions. Accordingly, our goal is to forward the predictive fit view by taking a step back and delineating two distinct prediction tasks for an item response model. The first prediction task, which we name "missing responses," is to predict the probability of a missing item response. The second prediction task, which we name "missing persons" is to predict the probability of all of the responses from a new, randomly drawn person. These two prediction tasks correspond to two predictive fit metrics, which we define as measures of how well an item response model predicts new data from the DGM.

**Organization.** We first give background on item response models and how they are typically compared in practice. Second, we describe the two possible prediction tasks which correspond to different definitions of out-of-sample for item response data. Third, we derive two predictive fit metrics based on these two definitions. We derive these metrics for the theoretical case when the DGM is known, such as in a simulation study. Fourth, we show the behavior and utility of these metrics in four simulation studies. To reexamine and extend @kang2007irt, we compare the prediction-maximizing model (according to the predictive fit metrics) to the model selected selected by common methods such as AIC and BIC. Lastly, we provide a real world example of model selection, which includes a description of how to use cross-validation to estimate the predictive fit metrics in practice.

# Item Response Models

Let \(Y\) represent an observed item response matrix. \(y_{ij}\) is an observed dichotomous item response where \(y_{ij} = 1\) indicates that the \(i\)th person responded correctly to the \(j\)th item and \(y_{ij} = 0\) indicates that they responded incorrectly. Item response theory provides a framework for modeling \(Y\). The fundamental building block of item response theory is the item response function (IRF) which gives the probability that a person will respond correctly to (or positively endorse) an item [@baker2004item]. The 3PL IRF is commonly used and is specified as
\begin{equation}
    \text{Pr}(y_{ij} = 1) = c_j + (1 - c_j) F(a_j \theta_i + b_j)
\end{equation}
where \(\theta_i\) is the \(i\)th person's ability; \(a_j\), \(b_j\), and \(c_j\) are the \(j\)th item's discrimination, easiness, and guessing parameters respectively; and \(F\) is the sigmoid function, \(F(x) = \dfrac{e^x}{1 + e^x}\). The two parameter logistic (2PL) and one parameter logistic (1PL) IRFs can be thought of as constrained forms of the 3PL IRF. The 2PL IRF constrains the guessing parameter \(c_j\) to \(0\). The 1PL IRF constrains the guessing parameter \(c_j\) to \(0\) and the discrimination parameter \(a_j\) to \(1\).\footnote{Typically, but not always, the specification of the IRF is the same for each item on an exam. For example, as is common, we refer to the case where each of the items has a 3PL IRF as a 3PL model.}

Item response models are most commonly estimated using marginal maximum likelihood estimation (MMLE), which estimates item parameters while treating person abilities as a nuisance parameter [@bock1983discrete; @casabianca2015irt]. MMLE was developed to remedy the fact that simultaneously estimating item and person parameters yields statistically inconsistent estimates as the number of persons goes to infinity [@bock1981marginal]. As such, MMLE estimates item parameters by maximizing the marginal log likelihood (MLL) of the model,
\begin{align}
\text{MLL}(\text{model}(Y))= \sum_{i}^{I} \log \int \left[\prod_{j=1}^{J} \hat{\text{Pr}}(y_{ij} | \bm{\hat\psi_j}, \bm{\theta}) \right] \hat g(\bm{\theta}) d\bm{\theta}
\end{align}
where $\bm{\hat\psi_j}$ is the vector of estimated item parameters and $\hat g(\bm{\theta})$ is the distribution of ability. Typically $\hat g(\bm{\theta})$ is assumed to be normally distributed with hyperparameters (e.g., mean and/or variance) estimated during model fitting. \footnote{For example, the mirt R package assumes that $\hat g(\bm{\theta})$ follows a multivariate normal distribution by default. When fitting a 1PL model, the mean is fixed to 0 and the variance is estimated [@chalmers2012mirt]. When fitting a 2PL model, the mean is fixed to 0 and the variance is fixed to 1 (these fixed ability parameters are compensated for by free estimation of item difficulties and item discriminations, respectively).} [@baker2004item]. If necessary, person ability estimates can be obtained using an estimation technique such as expected a-posteriori (EAP) or maximum a-posteriori (MAP) following MMLE of item parameters [@bock1983discrete].

## Item Response Model Selection In Practice

Item response models are typically compared using likelihood ratio tests (LRT) or information criterion such as AIC and BIC [@maydeu2013goodness]. Each of these methods is based on the model's marginalized log likelihood, $\text{MLL}(\text{model}(Y))$, and number of item parameters estimated in fitting the model, $M$. Consider two models fit to $Y$: $\text{model}_1(Y)$ and $\text{model}_2(Y)$ where the latter model has a greater number of parameters such that $M_2 > M_1$. An LRT compares these models by exploiting the fact that
\begin{align}
2 \cdot [\text{MLL}(\text{model}_2(Y)) - \text{MLL}(\text{model}_1(Y))]
\end{align}
follows a chi-squared distribution with $M_2 - M_1$ degrees of freedom [@andersen1973goodness; @baker2004item]. If the associated p-value is statistically significant (often at a 0.05 significance level), then it is concluded that $\text{model}_2(Y)$ fits better than $\text{model}_1(Y)$. On the other hand, AIC and BIC both add a penalty to the likelihood based on $M$:
\begin{align}
\text{AIC}(\text{model}) &= -2 \cdot \text{MLL}(\text{model}(Y)) + 2M \\
\text{BIC}(\text{model}) &= -2 \cdot \text{MLL}(\text{model}(Y)) + \log(I) \cdot M
\end{align}
where $I$ is the number of persons (i.e., rows) in $Y$. Lower values of AIC and BIC indicate better fit. With the goal of defining predictive fit metrics, we now take a step back to describe from first principles the missing responses and missing persons prediction tasks for item response models.
